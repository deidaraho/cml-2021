{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs. Unsupervised Training\n",
    "\n",
    "The sorts of problems we've seen so far in this class are examples of __supervised training__. \n",
    "\n",
    "Supervised training is training model on a dataset with both predictor and response variables in order to get a model that can predict the response in the future, when we supply just the predictors.\n",
    "\n",
    "There are also other kinds of problems, including __unsupervised training__, where we are modeling data for which we do not have labels (\"ground truth\" response variable data).\n",
    "\n",
    "#### How can we possibly build a model without any true response data?\n",
    "\n",
    "Most unsupervised learning is about discovering patterns in data -- \n",
    "\n",
    "1. either in relation it other data points (e.g., where are records mostly similar vs. where are outliers or anomalies in the data, something useful for fraud, instrusion, or process-failure detection)\n",
    "2. or in relation to some constant, external standards (e.g., we could learn about the contents of bright vs. dark photos by using a simple heuristic [instead of labeling] for deciding what is \"bright\"; we could then look for commonalities within the two groups)\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "The most common unsupervised patterns for business applications are forms of clustering, or determining which sets of data points are most similar.\n",
    "\n",
    "* As noted above, anomaly detection for fraud, quality control, etc. are a form of \"1 group\" clustering (where we measure coherence to the central normative group) or n-group clustering (where one large group is taken as normative, and others represent types of deviation)\n",
    "* Clustering algorithms are also widely used for customer segmentation, as part of a recommendation workflow, etc.\n",
    "* Clustering is often more subtle and tricky than it seems ... \"obvious\" groupings and allegiances can turn out to be a distraction, and most companies are unable to successfully address the many power-law-distributed clusters that make up the long tail of their customer base, even when most of their value derives from these smaller groups.\n",
    "* As with other algorithms, there are hyperparameters to be tuned -- i.e., subjective human input applied -- when performing unsupervised learning. I.e., even unsupervised learning is not 100% \"objective.\"\n",
    "\n",
    "#### K-Means Example\n",
    "\n",
    "A common clustering algoithm is K-Means, which attempts to partition the data into K (where a human supplies K as a hyperparameter) distinct clusters in a coherent way. Suppose we hypothesized that our diamond sales fell into 3 natural categories -- say, small and cheap; large, rare, and expensive; and some sort of middle group. We could run K-Means on the data ... and the we would need to carefully analyze the results to see if they match our hypothesis at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/diamonds.csv')\n",
    "df2 = df.drop(df.columns[0], axis=1)\n",
    "df3 = pd.get_dummies(df2)\n",
    "\n",
    "y = df3.iloc[:,3]\n",
    "X = df3.drop(df3.columns[3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_labels = KMeans(n_clusters=3).fit_predict(X)\n",
    "\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = pd.DataFrame(data=np.column_stack([cluster_labels, y]), columns=['cluster', 'price'])\n",
    "print([kdf[kdf.cluster==n]['price'].mean() for n in range(0,3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "Another popular form of unsupervised learning is called dimensionality reduction.\n",
    "\n",
    "If our featurized data records are points in a high-dimensional space, dimensionality reduction is the process of trying to find a good representation of that same data in lower-dimensional space. This is equivalent to representing the data with a smaller number of features, which may be more meaningful. We may even believe that all of the information we care about lies in a low-dimensional, well-behaved subspace of our original data (this is called the \"manifold hyptothesis\").\n",
    "\n",
    "The catch, of course, is that we'd like to not lose import aspects of the data when we do this, where \"important\" is defined as the information we need to make good classifications or regressions or other analytics conclusions.\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/aiOUoim.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be an __unsupervised learning__ activity when we use an algorithm to try and find the low-dimensional structure or approximation of our dataset, without any labels or special classes being defined.\n",
    "\n",
    "Not all dimensionality reduction is linear, as in the diagram above, but one popular linear algorithm is Principal Component Analysis or PCA.\n",
    "\n",
    "PCA aims to identify linear components of the data space which account for \"most\" of the variance in the data. We can choose how many components to retain, in order of decreasing variance (information).\n",
    "\n",
    "If you're crafting these representations yourself, why is it important? On the data engineering side, just as with feature selection, reduced dimensionality may offer a big reduction in data and model size, as well as a big increase in speed. On the other hand, performing PCA on a large dataset can be expensive, depending on the number of records, number of features, and algorithm used. Scalable singular-value decomposition (\"SVD\"), a matrix factorization, is one common approach when records have large numbers of features (e.g., in Apache Spark's data-parallel model: https://spark.apache.org/docs/2.3.0/mllib-dimensionality-reduction.html#singular-value-decomposition-svd)\n",
    "\n",
    "Here we'll take a look at a small example that illustrates the concept, using our iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of IRIS dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Dimensionality Reduction (Manifold Learning)\n",
    "\n",
    "__Manifold Hypothesis__\n",
    "\n",
    "Algorithms: https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Manifold_learning_algorithms\n",
    "\n",
    "*How might this work?*\n",
    "\n",
    "Example: https://umap-learn.readthedocs.io/en/latest/how_umap_works.html\n",
    "\n",
    "For an example with UMAP (and GPU acceleration!) see the GPU_vs_CPU_UMAP notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction of Discrete Data (Embeddings)\n",
    "\n",
    "Consider a high-cardinality set of discrete items. Tokens (as used in NLP) are a canonical example.\n",
    "\n",
    "Brute force approaches (a dimension per 1-hot encoded word) won't work. Shortcuts (count vectors, Hashing TF-IDF) can help but are not perfect. \n",
    "\n",
    "What if ... \n",
    "* we didn't have to spend dimensions on many unique words that mean the same thing?\n",
    "* we could normalize words by sense, or find similarity by sense?\n",
    "* even use the \"sense vectors\" of words across different languages? \n",
    "\n",
    "#### Let's discuss *Word Embeddings*\n",
    "\n",
    "__We'll start with word vectors__\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/iWzuElv.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But ... how?!\n",
    "\n",
    "There are several approaches to training word vectors. One is training them as __embedding matrices__ using __noise-contrastive estimation__ and n-ngram prediction tasks.\n",
    "\n",
    "Variations on this technique are fundamental to SOTA NLP work, and can even work across languages!\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/ig1Uytr.jpg\" width=900>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "04-Training-Dimensions-Streaming",
  "notebookId": 2375086480048713
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
