{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (and Feature Engineering) at Scale\n",
    "\n",
    "## Feature Engineering at Scale\n",
    "\n",
    "Some feature engineering operations are \"embarrassingly parallel\" so we can easily use any parallel processing scheduler (e.g., Spark, Dask) to apply them to data. SparkML calls these `Transformers`.\n",
    "\n",
    "Examples include:\n",
    "* Normalizing text (e.g., lowercasing, removing stop words)\n",
    "* Thresholding & Bucketing\n",
    "* Polynomial Expansion\n",
    "    \n",
    "Others require at least *some* information gained from looking at all (or a big chunk) of data. In the data-parallel terminology, we'd say these rely on some kind of \"reduce\" operation to acquire state, which is then used for the per-record processing. SparkML calls these `Estimators` (although SparkML Estimators also include parallel algorithms -- see below).\n",
    "\n",
    "Examples:\n",
    "* Standard scaling (we need the mean and std dev from at least a suitable sample of the data)\n",
    "* Category encoding, one-hot encoding (we need to find all the unique values of the field)\n",
    "* Deskewing (the Box-Cox deskew requires a parameter, \"lambda,\" which depends on the skewed data)\n",
    "\n",
    "These can require a bit of thought depending on what large-data processing system you're using. Most designed-for-big-data ML tools handle these issues in a sensible way, but it's good to check.\n",
    "\n",
    "## Model Training at Scale\n",
    "\n",
    "For most algorithms, __parallel, scale-out training requires a fundamentally different algorithm from the local/serial implementation__. So we cannot usually take a these traditional implementations (e.g., most of scikit-learn or R) and just magically run it on a cluster via Spark/Dask/etc.\n",
    "\n",
    "Many (most?) scale-out implementations rely on using a parallel implementation of a convex optimizer to try and find good parameter values for parametric models, e.g.\n",
    "* Stochastic Gradient Descent (SGD) and related optimizers (Adagrad, Nesterov, etc.) in deep learning frameworks like TensorFlow\n",
    "* Second-Order/Quasi-Newton methods for Generalized Linear Models in SparkML\n",
    "\n",
    "<img src='https://materials.s3.amazonaws.com/i/sgd.jpg' width=600 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints for parallelizing on multiple nodes with TensorFlow/PyTorch:\n",
    "* You may want to use Spark/Dask first for feature engineering\n",
    "* Use Horovod (https://github.com/horovod/horovod) for easy scaling\n",
    "* Use Petastorm (https://github.com/uber/petastorm) to convert columnar big data (Parquet) to dense row-based data that these tools prefer to consume\n",
    "\n",
    "So although we can't magically make our local implementations scale out, we can borrow the optimizers from these other systems -- like Spark, TensorFlow, or PyTorch -- to get scale-out training on any problem we can formulate as an optimization.\n",
    "\n",
    "And for almost all common cases, existing tools can help. For example, XGBoost has parallel (and GPU-parallel) implementations, so you can download and leverage those. \n",
    "\n",
    "Where we can't use these scale-out approaches, looks for tricks like statistical approximations. For example, k-Nearest-Neighbors doesn't scale in the naive implementation but, e.g., SparkML includes and Approximate Nearest Neighbors implementation which does scale (relying on locality-sensitive hashing).\n",
    "\n",
    "## Model Inference at Scale\n",
    "\n",
    "You have a model which you're happy with and you want to perform inference (or scoring) on a large dataset.\n",
    "\n",
    "This case is an easy (embarrassingly parallel) problem, which can be approached in whatever way you like. A key point to mention is that if you choose a good format to represent your featurization pipeline and model (e.g., PMML, PFA, ONNX) then you can perform inference in a system that is totally separate from the training environment.\n",
    "\n",
    "For example, you can:\n",
    "* Train a neural net with TensorFlow, but then use Spark to perform bulk inference\n",
    "* Use Spark to train a model, but then use, say Kubernetes, Docker, and Microsoft's ONNX Runtime for a scaling inference service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
