{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning\n",
    "\n",
    "aka \n",
    "\n",
    "* Predictive Analytics\n",
    "* Artificial Intelligence\n",
    "* Predictive Modeling\n",
    "\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the AI excitement now? \n",
    "\n",
    "#### What's new or changed?\n",
    "  \n",
    "* More data\n",
    "* More compute\n",
    "* Free and open-source commodity tooling\n",
    "  * Algorithms\n",
    "  * Wrapper frameworks\n",
    "  * Operational patterns (e.g., containerization, microservices, etc.)\n",
    "* More data engineers/analysts/scientists in most industries\n",
    "  * n.b., finance, pharma, travel, insurance, weather, government have been doing this for a long time but it has not always translated into a \"head start\"\n",
    "  \n",
    "#### What's the same?\n",
    "\n",
    "* \"Garbage In - Garbage Out\"\n",
    "* Mediocre statistical understanding, mathematical reasoning\n",
    "* Conflicting organizational priorities\n",
    "* Regulatory oversight and restrictions (for some industries/data)\n",
    "\n",
    "#### What are we still waiting for?\n",
    "\n",
    "* \"Automagic\" data preparation and modeling\n",
    "* Consensus on legal/ethical use of models, and regulatory frameworks\n",
    "* Automation of non-trivial human tasks\n",
    "  * \"Anything that a typical human can do with at most 1 sec of thought, can probably now or soon be automated with AI.\" -Andrew Ng\n",
    "* Consensus on roles, responsibilities, titles, training, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the applications of AI to business data?\n",
    "\n",
    "* More sales\n",
    "* Better sales (customer segmentation, pricing)\n",
    "* Better products/services/processes/operations\n",
    "* Fewer defects/failures\n",
    "* Increased velocity\n",
    "* Lower costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A business process model for AI: CRISP-DM\n",
    "\n",
    "__\"Cross-Industry Standard Process for Data Mining\"__\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/bqdknQ1.png\" width=400>\n",
    "\n",
    "CRISP-DM is not necessarily the right, complete solution for your business. But it's a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining & Understanding Data\n",
    "\n",
    "Authors at Google wrote a paper (https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf) featuring this widely used image:\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/MRtjDiB.png\" width=800>\n",
    "\n",
    "In a nutshell, just because we \"have\" tons of data from our transactions, sales channels, devices, suppliers, etc. does not make that data useful right away for AI.\n",
    "\n",
    "Cost-Benefit analysis should be used when determining the allocation of resources for new data-driven initiatives including AI.\n",
    "\n",
    "*Beware products and papers claiming that your data can be automatically processed, or that some AI model can learn from a very small amount of data.*\n",
    "\n",
    "For example, some claims about learning from little or no data are technically true and mathemathematically interesting for researchers. But they should be treated with extreme caution in most business analytics settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs. Test Data\n",
    "\n",
    "__Idea__: It's easy to cheat if you have the answers in front of you. Since machines have fast, accurate memory, it's easy for them to cheat if they have ever seen *any aspect* of the \"answers\" (or any signal that leaks information about them).\n",
    "\n",
    "What do we mean by \"cheating\"? What could go wrong?\n",
    "\n",
    "__The typical problem is a model that seems to do really well (posts great scores) when we're developing it but doesn't do so well in real life.__ Since our business goals depend on the model working on new data records, we want to avoid this scenario.\n",
    "\n",
    "For AI, we have to be careful to separate training data from test data, or we may not be able to figure out how well the model performs.\n",
    "\n",
    "__Terminology__: Training set; evaluation set; test set. \n",
    "\n",
    "Data leakage: any information about the data records which may be present when we are modeling/analyzing, but would not realistically be present when performing prediction or inference on new records.\n",
    "\n",
    "__Let's see an example of leakage__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: Diamonds\n",
    "\n",
    "This dataset of diamond sales (http://ggplot2.tidyverse.org/reference/diamonds.html) is of moderate size (~55,000 records) and resembles data records that occur in many business scenarios.\n",
    "\n",
    "For each of the diamond sales records, we have the following properties:\n",
    "* price: price in US dollars (\\\\$326-$18,823)\n",
    "* carat: weight of the diamond (0.2-5.01)\n",
    "* cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "* color: diamond colour, from J (worst) to D (best)\n",
    "* clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "* x: length in mm (0-10.74)\n",
    "* y: width in mm (0-58.9)\n",
    "* z: depth in mm (0-31.8)\n",
    "* depth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43-79)\n",
    "* table: width of top of diamond relative to widest point (43-95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/diamonds.csv')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"unnamed\" column is a row number in the dataset. It turns out that this row number -- which sounds like it should be meaningless -- actually leaks key data about the diamonds. \n",
    "\n",
    "Can you think of why this might be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='record #', ylabel='$')\n",
    "plt.plot(df.iloc[:,0], df['price'], ',') # , means just a pixel marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of the row number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "df2[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors / Response\n",
    "\n",
    "More terminology and key ideas:\n",
    "\n",
    "__\"Predictors\"__ are properties of our data records that we can learn from, and which typically are available when we want to make a prediction in the future.\n",
    "\n",
    "__Example: We would like to predict the rental price of an apartment based on its location, square footage, and number of bedrooms__\n",
    "\n",
    "In this example, the location, square footage, and number of bedrooms are all *predictors* because we can learn from them and we will usually know them when we want to predict a rental price of a new apartment listing in the future.\n",
    "\n",
    "__\"Response\"__ is the thing we want to predict -- it's available for learning, but __not__ availabile when we want to make a prediction in the future.\n",
    "\n",
    "In our example, the rental price would be the \"response variable\" ... we have historical records to train from, but when we apply our model in the future, we want the model to predict the (unknown, likely) rental price.\n",
    "\n",
    "\n",
    "\n",
    "## Business rules & \"Priors\" (aka prior belief, prior distribution)\n",
    "\n",
    "Many discussions of AI, as well as books and even famouse competitions (e.g., Kaggle) proceed solely via manipulating data with math, and make no reference to industry knowledge and business rules.\n",
    "\n",
    "__This phenomenon is *not* because AI is smarter than traditional business knowledge__ but rather because those AI resources are more focused in the mathematical abstractions over predictorsa. (Bayesian and some other specific modeling techniques are exceptions to this generalization.)\n",
    "\n",
    "In real life, you most definitely want to apply your knowledge of the business domain to improve your models' accuracy, as well as save lots of time and money forcing a model to \"learn\" things that are obviously true in your business.\n",
    "\n",
    "Many real world patterns or constraints can be applied to AI modeling and will usually improve the outcome (though we do want to be careful that we don't apply unnecessary or illegitimate assumptions).\n",
    "\n",
    "Sometimes this practice is described (or even implemented) as a \"prior distribution\" (or just \"prior\"). This refers to the distribution of data that we believe holds true before we've tried to learn from our data records. \n",
    "\n",
    "Some priors imply basic \"common sense\" -- like the idea that a crime rate or square footage can't be a negative number\n",
    "\n",
    "Others are domain-specific: e.g., considering rent in San Francisco, we might start out with the belief that every habitable 1-bedroom apartment costs more than $1500 per month. If we can bake this belief into the model prior to training, the result may better reflect real world prices. It's not magic -- it's just providing additional information to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression vs. Classification\n",
    "\n",
    "Most machine learning problems are either \"regression\" problems or \"classification\" problems.\n",
    "\n",
    "__Regression__ means that we would like to predict number or value from a continuous range. For example, predicing apartment price in dollars is a regression problem.\n",
    "\n",
    "__Classification__ means that our response variable takes on one of a discrete set of values. If we have some properties about a transaction, and we want to predict whether the transaction is fraudulent, that is a classification problem because the answer comes from a set of just two values: \"fraud\" or \"not-fraud.\" Handwriting recognition is also a classification problem: given a hand written character, we want to know which letter or word (from a specific set) it corresponds to.\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/L1vwRDL.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: k-Nearest-Neighbors (kNN)\n",
    "\n",
    "Let's discuss an algorithm. What is an algorithm? A set of steps for processing input to reach some goal. In this case, our goal is to build some sort of model that we can use to predict valuable things in the future.\n",
    "\n",
    "But what is a model? A model is an abstraction of a system and so can \"be\" almost anything. We're more concerned with how useful it is to us -- how well it can improve our business by making good predictions -- than the philosophy of exactly what it \"is.\"\n",
    "\n",
    "### kNN\n",
    "\n",
    "One really simple -- and surprisingly accurate -- model is called \"k Nearest Neighbors\" or kNN for short.\n",
    "\n",
    "What is kNN about? \n",
    "\n",
    "It's based on the idea that one way to make a prediction for a new record is to find some records we already know about, and which are as similar as possible to the new one. In our apartment example, one way to set a price for a new apartment -- and one used by real estate agents -- is to look at \"comparables\" which are other, similar apartments.\n",
    "\n",
    "How many similar items do we look for? That's the \"k\" and you get to decide what to use for k. In some problem domains, we might look at just one or two records, especially if we have a lot of examples so we can find some that are very similar. For real estate pricing, we might want to look at more -- perhaps 20 or 30 -- depending upon how complex the market is.\n",
    "\n",
    "Once we find the k most similar records, we might average their prices or use another formula to get our predicted price.\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/WbW58ty.png\">\n",
    "\n",
    "__So what's the model here?__\n",
    "\n",
    "In this case, the model is just all of the records we have. We don't really need to do anything with them except make sure they're converted to a form that allows us to measure \"nearest\" or \"most similar\" (which itself can be nontrivial)\n",
    "\n",
    "kNN, despite being simple, can work well ... but it is often impractical. Why? it works best when we have lots of data points with lots of variety ... and we need to keep all of those data points in order to be able to make a prediction. So it's often not compact nor fast. Put another way, it doesn't abstract or describe patterns in the data.\n",
    "\n",
    "### Before giving up on kNN, though, let's give it a try and see an actual example.\n",
    "\n",
    "__Using our diamonds dataset,__ let's work on predicting price from the other properties of the diamond.\n",
    "\n",
    "*Price will be the response variable*, while all the others are predictors.\n",
    "\n",
    "Since we are predicting a number that can take on any value (> $0 anyway), this is a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Feautres\n",
    "\n",
    "Now ... computers are good with numbers, but what about those words? (\"Premium\", \"Ideal\", etc.)\n",
    "\n",
    "It turns out that not only do we need to convert them to numbers, but we often want to do that in a way that treats them as totally separate properties.\n",
    "\n",
    "That is, we consider the \"Ideal\"-ness of a diamond totally separately from the \"Premium\"-ness of that diamond, etc., and of course each diamond only has one of those properties. This is called \"one-hot encoding\" (or sometimes \"dummy variable encoding\" or \"one of k encoding\").\n",
    "\n",
    "Why do we do this? Wouldn't it make more sense to measure the goodness-of-cut along a numeric scale, almost like the carat weight?\n",
    "\n",
    "In theory, yes -- and in some case your team may want to do that. But without putting in a lot of work (or having the business domain knowledge) to get that right, we can approximate with this encoding that is, in essence, just a math trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df2.cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Categorical(df2.cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In many cases, Pandas can do these steps for us (although the Categorical type is useful to know about & use)\n",
    "df3 = pd.get_dummies(df2)\n",
    "\n",
    "df3.iloc[:3, 7:18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll split out a \"test set\" -- remember we want to be able to evaluate the model on records that it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df3.price\n",
    "\n",
    "X = df3.drop(columns='price')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baselines\n",
    "\n",
    "We're almost ready to do some modeling! But modeling costs time and money, and may not produce useful results ... if we're not careful, it could even produce results that are counterproductive or illegal.\n",
    "\n",
    "So just like we might have test suites for software systems, we should ideally have test suites for models.\n",
    "\n",
    "Some of those suites will measure the model performance, or looks for patterns of bias.\n",
    "\n",
    "But the most basic item at all -- a sort of smoke test -- is a baseline model. The baseline model provides a point of comparison, to see if the model is learning anything.\n",
    "\n",
    "If you are refining a model, you might use an earlier version as a baseline. If you're starting from scratch and have no idea, you might use the constant mean of the response variable (\"average or expected result\") as a model. Even a made-up constant like 1 is better than no baseline.\n",
    "\n",
    "In this case we'll use the mean price of the diamonds as a (constant) baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our first \"baseline\" model just says for any diamond we might look at, its price is about $3900. Obviously this is usually going to be wrong, and often by a lot. But it's better than nother. Later we'll see how to compare a \"real\" model against this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up the model. As we said above, kNN is very simple ... but even complex models are easy to set up with this code library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=5)\n",
    "model = neigh.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, how did we do?\n",
    "\n",
    "For regression problems like this, we'll measure the accuracy of our predictions using RMSE (root mean squared error). This is a measure of \"how wrong\" we typically are in our predictions, measured in the units we are predicting (i.e., in this case, dollars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE %f\" % np.sqrt(mean_squared_error(y_test, y_pred)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So is that actually any good?\n",
    "\n",
    "One way to get an idea is to compare it to the mean and standard deviation of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.mean(), y_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, to simulate more closely a comparison to the baseline model, we could calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE %f\" % np.sqrt(mean_squared_error(y_test, np.full(len(y_test), y_train.mean()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "Let's try building another kind of model that learns a relationship in data by focusing on the \"most important\" data points, instead of all using all of them.\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/Svm_max_sep_hyperplane_with_margin.png\" width=500>\n",
    "\n",
    "[Lab: Support Vector Machine](./02b-SVM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric vs. Nonparametric models\n",
    "\n",
    "kNN and similar models that retain a lot of data can be useful. But in today's world of big data it can be impractical to work with a \"model\" that involves any large proportion of the full data set. \n",
    "\n",
    "It's hard to move around, hard to search or calculate over, and in some sense it misses out on a key goal of AI: learning and abstracting patterns from the data in an elegant, performant way, for future use.\n",
    "\n",
    "Similar to how an experienced real estate agent builds a \"mental model\" of her market, so that she can quickly guess a good price for an apartment without specifically memorizing thousands of listings, we'd like to come up with a compact yet expressive representation of patterns in our data.\n",
    "\n",
    "Models which comprise mathematical abstractions or summaries are called __parametric models__. The \"parameters\" in \"parametric\" are key values that encapsulate relationships in the data, just as the parameters *1.8* and *32* encapsulate the relationship between Fahrenheit and Celsius temperatures (where \\\\( F=1.8*C+32\\\\) ). Parametric and __nonparametric__ are not strictly exclusive nor opposites, but, generally, models that rely more directly on more of the data are considered nonparametric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Parametric Model: Linear Regression\n",
    "\n",
    "The canonical example of a parametric model is a linear regression model. Linear regression -- which you might have done by hand on a small amount of data in high school or a college stats class -- is simple, fast, robust, and performs reasonably well for many kinds of real-world data.\n",
    "\n",
    "In fact, linear regression is one of the two or three most widely used algorithms in the world for data modeling.\n",
    "\n",
    "Here's a simple version with one predictor and one response plotted against each other, along with a regression line:\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/gyP3KGA.png\">\n",
    "\n",
    "How does the computer (or the student) figure out where to draw that regression line? The goal is to minimize the __error__.\n",
    "\n",
    "What is the error? The difference (or distance) between the true value and the value predicted by the regression line:\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/cgvGCMg.jpg\" width=600>\n",
    "\n",
    "That might be getting into too much detail for this class, so let's just say we want to calculate the mathematically best-fit line.\n",
    "\n",
    "You can also notice that if the data itself does not embody a linear relationship, this approach may not work very well. Surprisingly, a lot of phenomena do have a large enough linear component that this algorithm often works. One thing that will help it fit complex data -- like your business records or our diamond sales -- is using more dimensions. That is, unlike the pictures here which just have one predictor (to make the pictures simple), we can use the same approach to calculate a response as a linear function of many dimensions. \n",
    "\n",
    "Let's fit a linear regression model to our diamonds dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lr = linear_model.LinearRegression()\n",
    "linear = lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linear.predict(X_test)\n",
    "print(\"RMSE %f\" % np.sqrt(mean_squared_error(y_test, y_pred)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model didn't fit quite as well as the kNN model (the RMSE here is larger, indicating our predictions are off by a few hundred more dollars). However, this model is very compact, since it is completely defined by about 27 parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: %s\" % linear.coef_)\n",
    "\n",
    "print(\"Intercept: %s\" % linear.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And making a prediction requires just multiplying and then adding 26 pairs of numbers, so it is lightning fast, even on the tiniest embedded IoT device. Alternatively, if we want to make billions of predictions, we could do that in a second with higher-end server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Powerplant Output \n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant\n",
    "\n",
    "About the business problem: peaker plant operation\n",
    "\n",
    "What is in this dataset? Just under 10,000 observations of:\n",
    "\n",
    "* Temperature (AT) in the range 1.81°C and 37.11°C\n",
    "* Ambient Pressure (AP) in the range 992.89-1033.30 millibar,\n",
    "* Relative Humidity (RH) in the range 25.56% to 100.16%\n",
    "* Exhaust Vacuum (V) in the range 25.36-81.56 cm Hg\n",
    "* Net hourly electrical energy output (PE) 420.26-495.76 MW\n",
    "\n",
    "What is the goal? To model output (PE) based other measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/powerplant.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, think about your intuition, experience, or \"domain knowledge\" that might apply -- even if you don't know about power generation, you may have some ideas about atmospheric pressure and temperature, and how they might affect a combustion-based power output.\n",
    "\n",
    "Test those ideas by building some plots. With just 4 predictors, you can make plots with all of them. Notice anything interesting?\n",
    "\n",
    "Try to build a linear regression model for power output. (Hint: you can cut/paste a lot of the code we've already used in this notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "name": "01-Intro-Data-Models",
  "notebookId": 2375086480048742
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
