{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "* Latent Factor Models\n",
    "* Trends + Metadata\n",
    "* Item/Content Similarity\n",
    "* Association Rules / FP-Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization Examples: Collaborative Filtering and Topic Modeling\n",
    "\n",
    "We'll look at basic topic modeling pattern which is:\n",
    "* Common in business scenarios beyond text\n",
    "* Relies on matrix factorization\n",
    "\n",
    "Nonnegative matrix factorization (NMF) is about finding two matrices which, when multiplied, approximate a given matrix.\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/NMF.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why would we want to do this?\n",
    "\n",
    "__Collaborative Filtering__\n",
    "\n",
    "1. Imagine the rows of V represent customers, and the columns of V represent products (physical goods or media, like movies or songs).\n",
    "\n",
    "2. Then we can imagine that the W matrix represents each customer's affinity to a set of latent factors (aspects of a product, like movie genre or clothing style).\n",
    "\n",
    "3. H represents the correlation between those factors and each product.\n",
    "\n",
    "4. It is easy to imagine that with a ton of \"factors\" we could get a really good breakdown of customers' likes and product aspects. But -- if we can get close to this with just a small number of factors -- then we will have a *compact, fast, inexpensive* way to recommend products to customers (or maybe even engineer products).\n",
    "\n",
    "This is really another flavor of dimensionality reduction -- we're again finding a low-dimensional representation of the linkage patterns between customers and products.\n",
    "\n",
    "The same exact approach can help us distill \"topics\" in natural language processing. If we imagine the rows of V as documents (or text instances) and the columns of V as terms, then the latent factors can be \"topics\" that unite terms via corresponding usages in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, \n",
    "* Stop words are words so common that we don't want them counted at all (like \"the\")\n",
    "* TF-IDF is a way of encoding a document into a vector, which counts the frequency of terms in the document, but divides by the common-ness of the terms across all documents\n",
    "  * E.g., \"have\" -- if it were included at all and not filtered out as a stop word -- would carry minimal weight because it occurs in most documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = dataset.data[:n_samples]\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Frobenius norm\" is the square-root-of-sum-of-squares, of a matrix\n",
    "  * In this case is represents a Euclidean distance between the target matrix and the product of the approximate factors, so we'd like it to be small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(name, magnitude) for (name, magnitude) in zip(tfidf_feature_names, nmf.components_[0]) if name < 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other approaches to topic modeling, from Naive Bayes models to Latent Dirichlet Allocation, to neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "0b-Recommender-Overview",
  "notebookId": 2375086480049128
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
